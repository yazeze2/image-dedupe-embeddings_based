{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03e800a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07480749",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPPORTED_FORMATS = [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\", \".gif\", \".webp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7de0e98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_images_metadata(root_dir: str) -> List[Dict]:\n",
    "    image_files = []\n",
    "    root = Path(root_dir)\n",
    "\n",
    "    for filepath in root.rglob(\"*\"):\n",
    "        if filepath.suffix.lower() in SUPPORTED_FORMATS:\n",
    "            try:\n",
    "                stat = filepath.stat()\n",
    "                image_files.append({\n",
    "                    \"path\": str(filepath.resolve()),\n",
    "                    \"name\": filepath.name,\n",
    "                    \"ext\": filepath.suffix.lower(),\n",
    "                    \"size\": stat.st_size,\n",
    "                    \"modified\": stat.st_mtime,\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {filepath}: {e}\")\n",
    "    \n",
    "    return image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2081e92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the gallery path from config.yaml\n",
    "with open(\"../config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "SOURCE_DIR = Path(config[\"gallery_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12b624fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1816 images\n"
     ]
    }
   ],
   "source": [
    "photos = scan_images_metadata(SOURCE_DIR)\n",
    "print(f\"Found {len(photos)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3acf45ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(photos)\n",
    "df.to_parquet(\"../data/scanned_images_metadata.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "777e2946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gallery size (on disk): 2.88 GB\n"
     ]
    }
   ],
   "source": [
    "total_size_bytes = df['size'].sum()\n",
    "total_size_gb = total_size_bytes / (1024 ** 3)\n",
    "print(f\"Gallery size (on disk): {total_size_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde1352c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e4c60d5",
   "metadata": {},
   "source": [
    "> - ### Preprocess sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94977ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(path: str, size=(224, 224)) -> np.ndarray:\n",
    "    try:\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        img = img.resize(size)\n",
    "        return np.array(img)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to preprocess {path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31ba55ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(file_list: List[str], size=(224, 224)) -> List[np.ndarray]:\n",
    "    results = []\n",
    "    for path in file_list:\n",
    "        preprocessed = preprocess_image(path, size)\n",
    "        if preprocessed is not None:\n",
    "            results.append(preprocessed)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96ad2905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, columns=5, size=(15, 5)):\n",
    "    n_images = len(images)\n",
    "    rows = (n_images + columns - 1) // columns\n",
    "    plt.figure(figsize=size)\n",
    "    \n",
    "    for i, img in enumerate(images):\n",
    "        plt.subplot(rows, columns, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "527f9e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1816, 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your previously scanned metadata\n",
    "df = pd.read_parquet(\"../data/scanned_images_metadata.parquet\")\n",
    "df = df[df[\"size\"] > 0].copy()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ad6978c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully preprocessed 100 images\n"
     ]
    }
   ],
   "source": [
    "# Select a small batch to test\n",
    "sample_paths = df[\"path\"].head(100).tolist()\n",
    "\n",
    "images = preprocess_batch(sample_paths)\n",
    "print(f\"Successfully preprocessed {len(images)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad43ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "126ca31e",
   "metadata": {},
   "source": [
    "> - ### Generate Embeddings with CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "642219c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e5463eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9ac3606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained CLIP model & processor\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c78407e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embeddings(images: list) -> torch.Tensor:\n",
    "    # Convert list of numpy arrays to list of PIL Images\n",
    "    pil_images = [Image.fromarray(img) for img in images]\n",
    "    \n",
    "    # Preprocess and tokenize\n",
    "    inputs = processor(images=pil_images, return_tensors=\"pt\", padding=True).to(device)\n",
    "    \n",
    "    # Get image embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model.get_image_features(**inputs)\n",
    "    \n",
    "    return outputs.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1507e82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ynany\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:540: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: torch.Size([100, 512])\n"
     ]
    }
   ],
   "source": [
    "embeddings = get_image_embeddings(images)\n",
    "print(f\"Embedding shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37742166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity_matrix(embeddings: torch.Tensor) -> np.ndarray:\n",
    "    embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "    sim_matrix = embeddings @ embeddings.T\n",
    "    return sim_matrix.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f16a2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicate_pairs(sim_matrix: np.ndarray, paths: list, threshold) -> pd.DataFrame:\n",
    "    n = sim_matrix.shape[0]\n",
    "    pairs = []\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            sim_score = sim_matrix[i, j]\n",
    "            if sim_score >= threshold:\n",
    "                pairs.append({\n",
    "                    \"img1\": paths[i],\n",
    "                    \"img2\": paths[j],\n",
    "                    \"similarity\": sim_score\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bce36f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29, 3)\n"
     ]
    }
   ],
   "source": [
    "sim_matrix = compute_similarity_matrix(embeddings)\n",
    "duplicate_df = find_duplicate_pairs(sim_matrix, sample_paths, threshold=0.95)\n",
    "print(duplicate_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2daa337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_duplicates(duplicate_df):\n",
    "    parent = {}\n",
    "\n",
    "    def find(x):\n",
    "        if parent.get(x, x) != x:\n",
    "            parent[x] = find(parent[x])\n",
    "        return parent.get(x, x)\n",
    "\n",
    "    def union(x, y):\n",
    "        px, py = find(x), find(y)\n",
    "        if px != py:\n",
    "            parent[py] = px\n",
    "\n",
    "    # Create union-find structure\n",
    "    for _, row in duplicate_df.iterrows():\n",
    "        union(row[\"img1\"], row[\"img2\"])\n",
    "\n",
    "    # Group images by their root parent\n",
    "    groups = defaultdict(list)\n",
    "    for img in set(duplicate_df[\"img1\"]).union(duplicate_df[\"img2\"]):\n",
    "        root = find(img)\n",
    "        groups[root].append(img)\n",
    "\n",
    "    return list(groups.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69a0ee62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17 duplicate groups\n"
     ]
    }
   ],
   "source": [
    "duplicate_groups = group_duplicates(duplicate_df)\n",
    "print(f\"Found {len(duplicate_groups)} duplicate groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ec0560f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 1: 5 images\n",
      "Group 2: 2 images\n",
      "Group 3: 2 images\n",
      "Group 4: 4 images\n",
      "Group 5: 3 images\n",
      "Group 6: 3 images\n",
      "Group 7: 2 images\n",
      "Group 8: 2 images\n",
      "Group 9: 2 images\n",
      "Group 10: 2 images\n",
      "Group 11: 2 images\n",
      "Group 12: 2 images\n",
      "Group 13: 2 images\n",
      "Group 14: 2 images\n",
      "Group 15: 2 images\n",
      "Group 16: 3 images\n",
      "Group 17: 2 images\n"
     ]
    }
   ],
   "source": [
    "# Show group sizes\n",
    "total=0\n",
    "for i, group in enumerate(duplicate_groups):\n",
    "    total+=len(group)\n",
    "    print(f\"Group {i+1}: {len(group)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b386993b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27ced4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b58bcd5d",
   "metadata": {},
   "source": [
    "> - ### Move or Copy Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aeff3c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_or_copy_duplicates(duplicate_groups, action=\"copy\", output_dir=\"../data/sample_duplicates\"):\n",
    "    \n",
    "    assert action in [\"copy\", \"move\"], \"Action must be 'copy' or 'move'\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    moved_files = []\n",
    "\n",
    "    for group in duplicate_groups:\n",
    "        if len(group) < 2:\n",
    "            continue\n",
    "\n",
    "        keep = group[0]\n",
    "        to_move = group[1:]\n",
    "\n",
    "        for path in to_move:\n",
    "            src = Path(path)\n",
    "            filename = src.name\n",
    "            dst = output_dir / filename\n",
    "\n",
    "            # Avoids overwriting\n",
    "            i = 1\n",
    "            while dst.exists():\n",
    "                dst = output_dir / f\"{src.stem}_{i}{src.suffix}\"\n",
    "                i += 1\n",
    "\n",
    "            try:\n",
    "                if action == \"copy\":\n",
    "                    shutil.copy2(src, dst)\n",
    "                else:\n",
    "                    shutil.move(src, dst)\n",
    "\n",
    "                moved_files.append({\"original\": str(src), \"duplicate\": str(dst)})\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to {action} {src} → {dst}: {e}\")\n",
    "\n",
    "    return moved_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63bded1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log = move_or_copy_duplicates(duplicate_groups, action=\"move\", output_dir=\"../data/sample_duplicates\")\n",
    "len(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9456c86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
